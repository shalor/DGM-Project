{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7322,"status":"ok","timestamp":1646677552503,"user":{"displayName":"אור שלום","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00402554675956823263"},"user_tz":-120},"id":"qXGXmTLhuqxh"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","def KL_(mu,sigma_sq):\n","    \"\"\"\n","    prior: p(z) ~ N(0,I)\n","    posterior approx.: q(z|x) ~ N(mu,sigma^2)\n","    :param mu: tensor of size [J,], holding the mean value\n","    :param sigma: tensor of size [J,], holding the s.d.\n","    :return: KL(q||p)\n","    \"\"\"\n","    return 0.5*torch.sum(1 + torch.log(1e-10 + sigma_sq) - mu**2 - sigma_sq) #1e-10 is added inside the log to set a min value.\n","\n","\n","class Model(nn.Module):\n","    def __init__(self, latent_dim,device):\n","        \"\"\"Initialize a VAE.\n","\n","        Args:\n","            latent_dim: dimension of embedding\n","            device: run on cpu or gpu\n","        \"\"\"\n","        super(Model, self).__init__()\n","        self.device = device\n","        self.latent_dim = latent_dim\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 32, 4, 1, 2),  # B,  32, 28, 28\n","            nn.ReLU(True),\n","            nn.Conv2d(32, 32, 4, 2, 1),  # B,  32, 14, 14\n","            nn.ReLU(True),\n","            nn.Conv2d(32, 64, 4, 2, 1),  # B,  64,  7, 7\n","        )\n","\n","        self.mu = nn.Linear(64 * 8 * 8, latent_dim)\n","        self.logvar = nn.Linear(64 * 8 * 8, latent_dim)\n","\n","        self.upsample = nn.Linear(latent_dim, 64 * 8 * 8)\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # B,  64,  14,  14\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(32, 32, 4, 2, 1, 1),  # B,  32, 28, 28\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(32, 3, 4, 1, 2),  # B, 1, 28, 28\n","            nn.Sigmoid()\n","        )\n","\n","\n","\n","    def sample(self,sample_size,mu=None,logvar=None):\n","        '''\n","        :param sample_size: Number of samples\n","        :param mu: z mean, None for prior (init with zeros)\n","        :param logvar: z logstd, None for prior (init with zeros)\n","        :return:\n","        '''\n","        if mu==None:\n","            mu = torch.zeros((sample_size,self.latent_dim)).to(self.device)\n","        if logvar == None:\n","            logvar = torch.zeros((sample_size,self.latent_dim)).to(self.device)\n","        #TODO\n","        if mu == None:  # Inference\n","            z = torch.randn((sample_size, self.latent_dim))\n","        else:\n","            z = self.z_sample(mu, logvar) #mu, e^logvar=var=sigma^2\n","        #z = self.z_sample(mu, logvar) #mu, e^logvar=var=sigma^2\n","        z = self.upsample(z)\n","        z = z.view([sample_size, 64, 8 , 8])\n","        return self.decoder(z)\n","\n","\n","    def z_sample(self, mu, logvar):\n","        return mu + torch.exp(0.5 * logvar) * torch.randn(mu.size()).to(self.device)\n","\n","\n","        self.prior_dist = torch.distributions.Normal(mu, torch.exp(0.5*logvar))\n","        return self.prior_dist.sample()\n","\n","\n","    def loss(self,x,recon,mu,logvar):\n","        \"\"\"\n","        :param x:\n","        :param recon: reconstructed\n","        :param mu:\n","        :param logvar:\n","        :return: loss = recon_loss - KL = -ELBO\n","        \"\"\"\n","        #\n","        # log_p_xi_zi = F.binary_cross_entropy(recon, x, reduction='none')\n","        # log_p_x_z = torch.sum(log_p_xi_zi, dim=1)\n","        # # KL\n","        # Ki_Li = -0.5 * (logvar - torch.exp(logvar) - torch.square(mu) + 1)  # -0.5*(2log(sig)-sig^2-mu^2+1)\n","        # KL = torch.sum(Ki_Li, dim=1)  # KL(p||q) = sum(KL(pi||qi))\n","        # return 0.5 * torch.sum(1 + torch.log(1e-10 + sigma_sq) - mu ** 2 - sigma_sq)  # 1e-10 is added inside the log to set a min value.\n","        # ELBO = log_p_x_z + KL\n","\n","        #TODO\n","        #BCE_loss = nn.BCELoss(reduction=\"sum\")\n","        #recon_loss = BCE_loss(recon, x)\n","        recon_loss = F.binary_cross_entropy(recon, x, reduction='sum')\n","        return recon_loss - KL_(mu, torch.exp(logvar)) #minus according to the definition in the paper\n","\n","    def forward(self, x):\n","        #TODO\n","        z = self.encoder(x)\n","        z_reshape = z.view([z.shape[0], -1])\n","        mu = self.mu(z_reshape)\n","        logvar = self.logvar(z_reshape)\n","        sample = self.upsample(self.z_sample(mu, logvar))\n","        sample = sample.view([sample.shape[0],64,8,8])\n","        x_recon = self.decoder(sample)\n","        return x_recon, mu, logvar\n","\n","#a = Model(100,'cpu')\n","#print(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20378,"status":"ok","timestamp":1645962977399,"user":{"displayName":"אור שלום","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00402554675956823263"},"user_tz":-120},"id":"k39KxbH7vEHF","outputId":"16c7f7fa-a512-4ab7-d530-1884dbd3a668"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\"\"\"Training procedure for NICE.\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import argparse\n","import torch, torchvision\n","from torchvision import transforms\n","import numpy as np\n","#from VAE import Model\n","import matplotlib.pyplot as plt\n","\n","def train(vae, trainloader, optimizer, epoch,device):\n","    vae.train()  # set to training mode\n","    #TODO\n","    import torchvision\n","    train_loss = 0\n","    for inputs_rgb, _ in trainloader:\n","        #if(inputs_rgb.size()[1]==3):\n","        #    inputs = torchvision.transforms.functional.rgb_to_grayscale(inputs_rgb)\n","        #else:\n","        inputs = inputs_rgb\n","        inputs = inputs.to(device)\n","        optimizer.zero_grad()\n","        output, mu, logvar = vae(inputs)\n","        loss = vae.loss(inputs, output, mu, logvar).mean() #loss = -ELBO\n","        #print(loss)\n","        train_loss += loss.item()\n","        #ELBO = - loss\n","        loss.backward()\n","        optimizer.step()\n","    train_loss = train_loss / len(trainloader.dataset)\n","    print(\"==> epoch %d: train_loss = %03.2f \" % (epoch, train_loss))\n","    return train_loss\n","\n","def test(vae, testloader, filename, epoch, device):\n","    vae.eval()  # set to inference mode\n","    test_loss = 0\n","    with torch.no_grad():\n","        # TODO\n","        if ((epoch +1)%50 ==0):\n","            samples = vae.sample(64).to(device)\n","            torchvision.utils.save_image(samples,'/content/drive/My Drive/VAE_Project_DGM/samples/' + filename + 'epoch%d.png' % epoch)\n","        for inputs_rgb, _ in testloader:\n","            #if(inputs_rgb.size()[1]==3):\n","            #    inputs = torchvision.transforms.functional.rgb_to_grayscale(inputs_rgb)\n","            #else:\n","            inputs = inputs_rgb\n","            inputs = inputs.to(device)\n","            output, mu, logvar = vae(inputs)\n","            test_loss += vae.loss(inputs, output, mu, logvar).item()\n","    test_loss /= len(testloader.dataset)\n","    print(\"==> epoch %d: test_loss = %03.2f \" % (epoch, test_loss))\n","    return test_loss\n","\n","def dequantization(x): #I have an issue with transforms.Lambda pickle, so I change to this function\n","    return x + torch.zeros_like(x).uniform_(0., 1./256.)\n","\n","def main(dataset = \"cifar10\", batch_size = 128, epochs = 50, sample_size = 64, latent_dim = 100, lr = 1e-3,continue_from = 0):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    transform  = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Lambda(dequantization), #dequantization\n","        transforms.Normalize((0.,), (257./256.,)), #rescales to [0,1]\n","\n","    ])\n","\n","    if dataset == 'mnist':\n","        trainset = torchvision.datasets.MNIST(root='./data/MNIST',\n","            train=True, download=True, transform=transform)\n","        trainloader = torch.utils.data.DataLoader(trainset,\n","            batch_size=batch_size, shuffle=True, num_workers=2)\n","        testset = torchvision.datasets.MNIST(root='./data/MNIST',\n","            train=False, download=True, transform=transform)\n","        testloader = torch.utils.data.DataLoader(testset,\n","            batch_size=batch_size, shuffle=False, num_workers=2)\n","    elif dataset == 'fashion-mnist':\n","        trainset = torchvision.datasets.FashionMNIST(root='~/torch/data/FashionMNIST',\n","            train=True, download=True, transform=transform)\n","        trainloader = torch.utils.data.DataLoader(trainset,\n","            batch_size=batch_size, shuffle=True, num_workers=2)\n","        testset = torchvision.datasets.FashionMNIST(root='./data/FashionMNIST',\n","            train=False, download=True, transform=transform)\n","        testloader = torch.utils.data.DataLoader(testset,\n","            batch_size=batch_size, shuffle=False, num_workers=2)\n","    elif dataset == 'cifar10':\n","        trainset = torchvision.datasets.CIFAR10(root='~/torch/data/CIFAR10',\n","            train=True, download=True, transform=transform)\n","        trainloader = torch.utils.data.DataLoader(trainset,\n","            batch_size=batch_size, shuffle=True, num_workers=2)\n","        testset = torchvision.datasets.CIFAR10(root='./data/CIFAR10',\n","            train=False, download=True, transform=transform)\n","        testloader = torch.utils.data.DataLoader(testset,\n","            batch_size=batch_size, shuffle=False, num_workers=2)\n","    else:\n","        raise ValueError('Dataset not implemented')\n","\n","\n","\n","    vae = Model(latent_dim=latent_dim,device=device).to(device)\n","    optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n","    #TODO\n","    filename = '%s_' % dataset \\\n","            + 'batch%d_' % batch_size \\\n","            + 'mid%d_' % latent_dim \\\n","            + 'epoch%d_' % epochs\n","    train_loss_list = []\n","    test_loss_list = []\n","    \n","    if (continue_from > 0):\n","      checkpoint = torch.load(\"/content/drive/My Drive/VAE_Project_DGM/models/cifar10_batch128_mid100_epoch3250__VAE.pt\",map_location=torch.device(device))\n","      #state = {'epoch': n_epoch + 1, 'state_dict': vae.state_dict(),'optimizer': optimizer.state_dict()}\n","      vae.load_state_dict(checkpoint['state_dict'])\n","      optimizer.load_state_dict(checkpoint['optimizer'])\n","      #stopped_at_epoch = continue_from #checkpoint['epoch']    \n","    \n","    for n_epoch in range(continue_from,epochs):\n","        train_loss_list.append(train(vae, trainloader, optimizer, n_epoch, device))\n","        test_loss_list.append(test(vae, testloader, filename, n_epoch, device))\n","\n","        if ((n_epoch +1)%50 ==0):\n","          filename2 = '%s_' % dataset \\\n","            + 'batch%d_' % batch_size \\\n","            + 'mid%d_' % latent_dim \\\n","            + 'epoch%d_' % (n_epoch+1)\n","          state = {'epoch': n_epoch + 1, 'state_dict': vae.state_dict(),'optimizer': optimizer.state_dict()}\n","          torch.save(state, '/content/drive/My Drive/VAE_Project_DGM/models/' + filename2 + '_VAE.pt')\n","    print(train_loss_list)\n","    print(test_loss_list)\n","\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--dataset',\n","                        help='dataset to be modeled.',\n","                        type=str,\n","                        default='cifar10')\n","    parser.add_argument('--batch_size',\n","                        help='number of images in a mini-batch.',\n","                        type=int,\n","                        default=128)\n","    parser.add_argument('--epochs',\n","                        help='maximum number of iterations.',\n","                        type=int,\n","                        default=1500)\n","    parser.add_argument('--sample_size',\n","                        help='number of images to generate.',\n","                        type=int,\n","                        default=64)\n","\n","    parser.add_argument('--latent_dim',\n","                        help='.',\n","                        type=int,\n","                        default=100)\n","    parser.add_argument('--lr',\n","                        help='initial learning rate.',\n","                        type=float,\n","                        default=1e-3)\n","\n","    #args = parser.parse_args()\n","    #main(dataset = \"cifar10\", batch_size = 128, epochs = 50, sample_size = 64, latent_dim = 100, lr = 1e-3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fa761cc6025c40e0943c6d0be1729a38","ec3e5c3fc94d4afa8bdefd39776896ff","6103acd3a3524c52a4a5ae79b953d228","e46b2168742f40c0b6482461f1c6ec87","7ef82d752ce14379b28f923656bea697","22226cc7c36e40ceaaab9bb1a4b1dcb6","b2618cb15bd24dbe8b1ed7521cbe7ddf","b8f8befc40334b93a3392e936b549252","6e3bdcb6a2324f9aa9797f11205538fe","58f959661f5141dc97a6a4ec10f87684","9a76cf42703547dbac9cda50d36f5726","bd1f8a975e6c44ca97c0f90e0c20c8d1","9d774cddd7d042f889f79d490f26eee5","42984c36f0eb45f49750e5052ea9ff4e","fe3dae1766694231a3f9bbab713f648c","6c93e7bfa72248a6a3270787a4fcfd96","2f9c753e6c1643ebacbe35fb54268ada","a5f0126280124811b09ea7f25727a28c","aea01ef94973470380a20a44d9abc585","df08e359128c404d9d6df0d7ec86d1a3","f9cc41387f0a4622aabc55bb84729827","609a16fc5c724553a8b5d47f5ee32aa8"]},"id":"TZV21Qqg0V3A","outputId":"c862b2d8-f537-47d0-a2d5-45f20c72bc61"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/torch/data/CIFAR10/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa761cc6025c40e0943c6d0be1729a38","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Extracting /root/torch/data/CIFAR10/cifar-10-python.tar.gz to /root/torch/data/CIFAR10\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/CIFAR10/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd1f8a975e6c44ca97c0f90e0c20c8d1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["Extracting ./data/CIFAR10/cifar-10-python.tar.gz to ./data/CIFAR10\n","==> epoch 3250: train_loss = 1814.97 \n","==> epoch 3250: test_loss = 1821.10 \n","==> epoch 3251: train_loss = 1815.04 \n","==> epoch 3251: test_loss = 1821.21 \n","==> epoch 3252: train_loss = 1815.03 \n","==> epoch 3252: test_loss = 1821.62 \n","==> epoch 3253: train_loss = 1815.08 \n","==> epoch 3253: test_loss = 1820.86 \n","==> epoch 3254: train_loss = 1814.95 \n","==> epoch 3254: test_loss = 1821.55 \n","==> epoch 3255: train_loss = 1815.01 \n","==> epoch 3255: test_loss = 1821.15 \n","==> epoch 3256: train_loss = 1815.06 \n","==> epoch 3256: test_loss = 1821.03 \n","==> epoch 3257: train_loss = 1815.00 \n","==> epoch 3257: test_loss = 1820.95 \n","==> epoch 3258: train_loss = 1815.00 \n","==> epoch 3258: test_loss = 1821.25 \n","==> epoch 3259: train_loss = 1815.05 \n","==> epoch 3259: test_loss = 1821.02 \n","==> epoch 3260: train_loss = 1814.98 \n","==> epoch 3260: test_loss = 1821.88 \n","==> epoch 3261: train_loss = 1815.01 \n","==> epoch 3261: test_loss = 1821.07 \n","==> epoch 3262: train_loss = 1815.01 \n","==> epoch 3262: test_loss = 1820.85 \n","==> epoch 3263: train_loss = 1815.11 \n","==> epoch 3263: test_loss = 1821.72 \n","==> epoch 3264: train_loss = 1815.03 \n","==> epoch 3264: test_loss = 1821.11 \n","==> epoch 3265: train_loss = 1814.99 \n","==> epoch 3265: test_loss = 1820.84 \n","==> epoch 3266: train_loss = 1815.07 \n","==> epoch 3266: test_loss = 1822.25 \n","==> epoch 3267: train_loss = 1815.01 \n","==> epoch 3267: test_loss = 1821.13 \n","==> epoch 3268: train_loss = 1814.97 \n","==> epoch 3268: test_loss = 1821.03 \n","==> epoch 3269: train_loss = 1815.05 \n","==> epoch 3269: test_loss = 1821.18 \n","==> epoch 3270: train_loss = 1814.99 \n","==> epoch 3270: test_loss = 1821.11 \n","==> epoch 3271: train_loss = 1815.03 \n","==> epoch 3271: test_loss = 1821.23 \n","==> epoch 3272: train_loss = 1814.97 \n","==> epoch 3272: test_loss = 1821.23 \n","==> epoch 3273: train_loss = 1815.06 \n","==> epoch 3273: test_loss = 1820.93 \n","==> epoch 3274: train_loss = 1815.02 \n","==> epoch 3274: test_loss = 1821.25 \n","==> epoch 3275: train_loss = 1815.07 \n","==> epoch 3275: test_loss = 1821.07 \n","==> epoch 3276: train_loss = 1815.03 \n","==> epoch 3276: test_loss = 1821.03 \n","==> epoch 3277: train_loss = 1815.01 \n","==> epoch 3277: test_loss = 1821.19 \n","==> epoch 3278: train_loss = 1814.97 \n","==> epoch 3278: test_loss = 1821.17 \n","==> epoch 3279: train_loss = 1815.09 \n","==> epoch 3279: test_loss = 1821.62 \n","==> epoch 3280: train_loss = 1814.98 \n","==> epoch 3280: test_loss = 1822.03 \n","==> epoch 3281: train_loss = 1815.08 \n","==> epoch 3281: test_loss = 1821.15 \n","==> epoch 3282: train_loss = 1815.05 \n","==> epoch 3282: test_loss = 1821.06 \n","==> epoch 3283: train_loss = 1814.98 \n","==> epoch 3283: test_loss = 1821.22 \n","==> epoch 3284: train_loss = 1814.94 \n","==> epoch 3284: test_loss = 1821.51 \n","==> epoch 3285: train_loss = 1814.92 \n","==> epoch 3285: test_loss = 1821.03 \n","==> epoch 3286: train_loss = 1815.03 \n","==> epoch 3286: test_loss = 1821.46 \n","==> epoch 3287: train_loss = 1814.99 \n","==> epoch 3287: test_loss = 1821.08 \n","==> epoch 3288: train_loss = 1815.07 \n","==> epoch 3288: test_loss = 1821.21 \n","==> epoch 3289: train_loss = 1815.03 \n","==> epoch 3289: test_loss = 1821.16 \n","==> epoch 3290: train_loss = 1815.00 \n","==> epoch 3290: test_loss = 1821.18 \n","==> epoch 3291: train_loss = 1815.08 \n","==> epoch 3291: test_loss = 1821.48 \n","==> epoch 3292: train_loss = 1815.00 \n","==> epoch 3292: test_loss = 1820.98 \n","==> epoch 3293: train_loss = 1815.02 \n","==> epoch 3293: test_loss = 1821.08 \n","==> epoch 3294: train_loss = 1815.00 \n","==> epoch 3294: test_loss = 1821.06 \n","==> epoch 3295: train_loss = 1814.93 \n","==> epoch 3295: test_loss = 1821.40 \n","==> epoch 3296: train_loss = 1814.99 \n","==> epoch 3296: test_loss = 1821.22 \n","==> epoch 3297: train_loss = 1815.02 \n","==> epoch 3297: test_loss = 1821.00 \n","==> epoch 3298: train_loss = 1815.01 \n","==> epoch 3298: test_loss = 1821.31 \n","==> epoch 3299: train_loss = 1815.02 \n","==> epoch 3299: test_loss = 1820.87 \n","==> epoch 3300: train_loss = 1814.96 \n","==> epoch 3300: test_loss = 1821.51 \n","==> epoch 3301: train_loss = 1815.07 \n","==> epoch 3301: test_loss = 1821.25 \n","==> epoch 3302: train_loss = 1815.04 \n","==> epoch 3302: test_loss = 1821.09 \n","==> epoch 3303: train_loss = 1815.00 \n","==> epoch 3303: test_loss = 1821.07 \n","==> epoch 3304: train_loss = 1815.07 \n","==> epoch 3304: test_loss = 1821.15 \n","==> epoch 3305: train_loss = 1814.99 \n","==> epoch 3305: test_loss = 1820.89 \n","==> epoch 3306: train_loss = 1814.97 \n","==> epoch 3306: test_loss = 1821.10 \n","==> epoch 3307: train_loss = 1815.12 \n","==> epoch 3307: test_loss = 1820.97 \n","==> epoch 3308: train_loss = 1815.06 \n","==> epoch 3308: test_loss = 1821.20 \n","==> epoch 3309: train_loss = 1815.06 \n","==> epoch 3309: test_loss = 1822.07 \n","==> epoch 3310: train_loss = 1814.94 \n","==> epoch 3310: test_loss = 1820.85 \n","==> epoch 3311: train_loss = 1814.96 \n","==> epoch 3311: test_loss = 1821.23 \n","==> epoch 3312: train_loss = 1815.00 \n","==> epoch 3312: test_loss = 1821.14 \n","==> epoch 3313: train_loss = 1814.99 \n","==> epoch 3313: test_loss = 1821.31 \n","==> epoch 3314: train_loss = 1815.04 \n","==> epoch 3314: test_loss = 1821.53 \n","==> epoch 3315: train_loss = 1815.01 \n","==> epoch 3315: test_loss = 1821.02 \n","==> epoch 3316: train_loss = 1814.96 \n","==> epoch 3316: test_loss = 1821.46 \n","==> epoch 3317: train_loss = 1815.00 \n","==> epoch 3317: test_loss = 1821.73 \n","==> epoch 3318: train_loss = 1815.00 \n","==> epoch 3318: test_loss = 1821.11 \n","==> epoch 3319: train_loss = 1814.99 \n","==> epoch 3319: test_loss = 1821.19 \n","==> epoch 3320: train_loss = 1815.05 \n","==> epoch 3320: test_loss = 1820.95 \n","==> epoch 3321: train_loss = 1815.00 \n","==> epoch 3321: test_loss = 1821.13 \n","==> epoch 3322: train_loss = 1815.00 \n","==> epoch 3322: test_loss = 1821.75 \n","==> epoch 3323: train_loss = 1815.03 \n","==> epoch 3323: test_loss = 1821.58 \n","==> epoch 3324: train_loss = 1814.96 \n","==> epoch 3324: test_loss = 1821.23 \n","==> epoch 3325: train_loss = 1814.98 \n","==> epoch 3325: test_loss = 1820.98 \n","==> epoch 3326: train_loss = 1815.04 \n","==> epoch 3326: test_loss = 1820.95 \n","==> epoch 3327: train_loss = 1815.04 \n","==> epoch 3327: test_loss = 1821.23 \n","==> epoch 3328: train_loss = 1815.08 \n","==> epoch 3328: test_loss = 1821.30 \n","==> epoch 3329: train_loss = 1814.97 \n","==> epoch 3329: test_loss = 1821.28 \n","==> epoch 3330: train_loss = 1815.04 \n","==> epoch 3330: test_loss = 1821.46 \n","==> epoch 3331: train_loss = 1814.95 \n","==> epoch 3331: test_loss = 1821.92 \n","==> epoch 3332: train_loss = 1815.04 \n","==> epoch 3332: test_loss = 1821.80 \n","==> epoch 3333: train_loss = 1815.10 \n","==> epoch 3333: test_loss = 1821.34 \n","==> epoch 3334: train_loss = 1814.99 \n","==> epoch 3334: test_loss = 1821.12 \n","==> epoch 3335: train_loss = 1815.08 \n","==> epoch 3335: test_loss = 1821.36 \n","==> epoch 3336: train_loss = 1815.00 \n","==> epoch 3336: test_loss = 1821.15 \n","==> epoch 3337: train_loss = 1814.95 \n","==> epoch 3337: test_loss = 1821.42 \n","==> epoch 3338: train_loss = 1815.03 \n","==> epoch 3338: test_loss = 1821.05 \n","==> epoch 3339: train_loss = 1815.00 \n","==> epoch 3339: test_loss = 1821.07 \n","==> epoch 3340: train_loss = 1814.94 \n","==> epoch 3340: test_loss = 1821.32 \n","==> epoch 3341: train_loss = 1815.09 \n","==> epoch 3341: test_loss = 1821.11 \n","==> epoch 3342: train_loss = 1814.96 \n","==> epoch 3342: test_loss = 1821.15 \n","==> epoch 3343: train_loss = 1814.99 \n","==> epoch 3343: test_loss = 1821.04 \n","==> epoch 3344: train_loss = 1815.03 \n","==> epoch 3344: test_loss = 1821.45 \n","==> epoch 3345: train_loss = 1814.98 \n","==> epoch 3345: test_loss = 1821.06 \n","==> epoch 3346: train_loss = 1814.97 \n","==> epoch 3346: test_loss = 1821.19 \n","==> epoch 3347: train_loss = 1815.05 \n","==> epoch 3347: test_loss = 1821.00 \n","==> epoch 3348: train_loss = 1815.01 \n","==> epoch 3348: test_loss = 1821.15 \n","==> epoch 3349: train_loss = 1815.01 \n","==> epoch 3349: test_loss = 1821.12 \n","==> epoch 3350: train_loss = 1815.09 \n","==> epoch 3350: test_loss = 1821.10 \n","==> epoch 3351: train_loss = 1815.00 \n","==> epoch 3351: test_loss = 1821.05 \n","==> epoch 3352: train_loss = 1814.94 \n","==> epoch 3352: test_loss = 1821.71 \n","==> epoch 3353: train_loss = 1814.99 \n","==> epoch 3353: test_loss = 1821.79 \n","==> epoch 3354: train_loss = 1815.00 \n","==> epoch 3354: test_loss = 1821.59 \n","==> epoch 3355: train_loss = 1814.99 \n","==> epoch 3355: test_loss = 1821.23 \n","==> epoch 3356: train_loss = 1814.92 \n","==> epoch 3356: test_loss = 1821.27 \n","==> epoch 3357: train_loss = 1814.98 \n","==> epoch 3357: test_loss = 1821.50 \n","==> epoch 3358: train_loss = 1815.07 \n","==> epoch 3358: test_loss = 1821.11 \n","==> epoch 3359: train_loss = 1814.96 \n","==> epoch 3359: test_loss = 1821.12 \n","==> epoch 3360: train_loss = 1815.00 \n","==> epoch 3360: test_loss = 1821.26 \n","==> epoch 3361: train_loss = 1814.98 \n","==> epoch 3361: test_loss = 1821.32 \n","==> epoch 3362: train_loss = 1814.97 \n","==> epoch 3362: test_loss = 1820.92 \n","==> epoch 3363: train_loss = 1815.03 \n","==> epoch 3363: test_loss = 1821.11 \n","==> epoch 3364: train_loss = 1815.06 \n","==> epoch 3364: test_loss = 1821.18 \n","==> epoch 3365: train_loss = 1814.98 \n","==> epoch 3365: test_loss = 1821.32 \n","==> epoch 3366: train_loss = 1814.98 \n","==> epoch 3366: test_loss = 1821.10 \n","==> epoch 3367: train_loss = 1814.98 \n","==> epoch 3367: test_loss = 1821.04 \n","==> epoch 3368: train_loss = 1814.96 \n","==> epoch 3368: test_loss = 1821.11 \n","==> epoch 3369: train_loss = 1814.96 \n","==> epoch 3369: test_loss = 1821.49 \n","==> epoch 3370: train_loss = 1815.06 \n","==> epoch 3370: test_loss = 1821.35 \n","==> epoch 3371: train_loss = 1815.02 \n","==> epoch 3371: test_loss = 1821.34 \n","==> epoch 3372: train_loss = 1814.99 \n","==> epoch 3372: test_loss = 1821.01 \n","==> epoch 3373: train_loss = 1814.98 \n","==> epoch 3373: test_loss = 1821.09 \n","==> epoch 3374: train_loss = 1815.10 \n","==> epoch 3374: test_loss = 1821.62 \n","==> epoch 3375: train_loss = 1814.98 \n","==> epoch 3375: test_loss = 1821.47 \n","==> epoch 3376: train_loss = 1815.00 \n","==> epoch 3376: test_loss = 1821.51 \n","==> epoch 3377: train_loss = 1815.00 \n","==> epoch 3377: test_loss = 1821.58 \n","==> epoch 3378: train_loss = 1815.09 \n","==> epoch 3378: test_loss = 1821.13 \n","==> epoch 3379: train_loss = 1814.95 \n","==> epoch 3379: test_loss = 1820.91 \n","==> epoch 3380: train_loss = 1815.02 \n","==> epoch 3380: test_loss = 1821.75 \n","==> epoch 3381: train_loss = 1815.05 \n","==> epoch 3381: test_loss = 1821.14 \n","==> epoch 3382: train_loss = 1814.95 \n","==> epoch 3382: test_loss = 1821.13 \n","==> epoch 3383: train_loss = 1815.03 \n","==> epoch 3383: test_loss = 1821.51 \n","==> epoch 3384: train_loss = 1814.94 \n","==> epoch 3384: test_loss = 1821.17 \n","==> epoch 3385: train_loss = 1814.95 \n","==> epoch 3385: test_loss = 1821.58 \n","==> epoch 3386: train_loss = 1814.99 \n","==> epoch 3386: test_loss = 1821.30 \n","==> epoch 3387: train_loss = 1815.01 \n","==> epoch 3387: test_loss = 1821.73 \n","==> epoch 3388: train_loss = 1815.06 \n","==> epoch 3388: test_loss = 1821.77 \n","==> epoch 3389: train_loss = 1815.04 \n","==> epoch 3389: test_loss = 1821.16 \n","==> epoch 3390: train_loss = 1815.07 \n","==> epoch 3390: test_loss = 1821.36 \n","==> epoch 3391: train_loss = 1815.01 \n","==> epoch 3391: test_loss = 1821.15 \n","==> epoch 3392: train_loss = 1814.97 \n","==> epoch 3392: test_loss = 1821.00 \n","==> epoch 3393: train_loss = 1815.00 \n","==> epoch 3393: test_loss = 1821.05 \n","==> epoch 3394: train_loss = 1814.98 \n","==> epoch 3394: test_loss = 1821.24 \n","==> epoch 3395: train_loss = 1814.97 \n","==> epoch 3395: test_loss = 1821.25 \n","==> epoch 3396: train_loss = 1815.06 \n","==> epoch 3396: test_loss = 1821.35 \n","==> epoch 3397: train_loss = 1814.96 \n","==> epoch 3397: test_loss = 1822.57 \n","==> epoch 3398: train_loss = 1815.01 \n","==> epoch 3398: test_loss = 1821.91 \n","==> epoch 3399: train_loss = 1815.03 \n","==> epoch 3399: test_loss = 1821.08 \n","==> epoch 3400: train_loss = 1815.00 \n","==> epoch 3400: test_loss = 1821.40 \n","==> epoch 3401: train_loss = 1814.99 \n","==> epoch 3401: test_loss = 1821.70 \n","==> epoch 3402: train_loss = 1814.98 \n","==> epoch 3402: test_loss = 1821.19 \n","==> epoch 3403: train_loss = 1814.97 \n","==> epoch 3403: test_loss = 1821.45 \n","==> epoch 3404: train_loss = 1815.00 \n","==> epoch 3404: test_loss = 1821.65 \n","==> epoch 3405: train_loss = 1815.03 \n","==> epoch 3405: test_loss = 1822.01 \n","==> epoch 3406: train_loss = 1815.00 \n","==> epoch 3406: test_loss = 1821.01 \n","==> epoch 3407: train_loss = 1815.01 \n","==> epoch 3407: test_loss = 1821.86 \n","==> epoch 3408: train_loss = 1814.92 \n","==> epoch 3408: test_loss = 1821.07 \n","==> epoch 3409: train_loss = 1815.04 \n","==> epoch 3409: test_loss = 1821.08 \n","==> epoch 3410: train_loss = 1815.10 \n","==> epoch 3410: test_loss = 1820.92 \n","==> epoch 3411: train_loss = 1814.98 \n","==> epoch 3411: test_loss = 1821.16 \n","==> epoch 3412: train_loss = 1815.09 \n","==> epoch 3412: test_loss = 1822.09 \n","==> epoch 3413: train_loss = 1814.90 \n","==> epoch 3413: test_loss = 1821.46 \n","==> epoch 3414: train_loss = 1814.97 \n","==> epoch 3414: test_loss = 1821.48 \n","==> epoch 3415: train_loss = 1814.95 \n","==> epoch 3415: test_loss = 1821.10 \n","==> epoch 3416: train_loss = 1814.96 \n","==> epoch 3416: test_loss = 1820.98 \n","==> epoch 3417: train_loss = 1814.99 \n","==> epoch 3417: test_loss = 1820.98 \n","==> epoch 3418: train_loss = 1815.08 \n","==> epoch 3418: test_loss = 1821.12 \n","==> epoch 3419: train_loss = 1814.94 \n","==> epoch 3419: test_loss = 1821.40 \n","==> epoch 3420: train_loss = 1815.06 \n","==> epoch 3420: test_loss = 1821.56 \n","==> epoch 3421: train_loss = 1814.96 \n","==> epoch 3421: test_loss = 1821.07 \n","==> epoch 3422: train_loss = 1814.94 \n","==> epoch 3422: test_loss = 1820.85 \n","==> epoch 3423: train_loss = 1814.98 \n","==> epoch 3423: test_loss = 1821.13 \n","==> epoch 3424: train_loss = 1815.01 \n","==> epoch 3424: test_loss = 1821.61 \n","==> epoch 3425: train_loss = 1815.06 \n","==> epoch 3425: test_loss = 1821.38 \n","==> epoch 3426: train_loss = 1814.97 \n","==> epoch 3426: test_loss = 1821.10 \n","==> epoch 3427: train_loss = 1814.95 \n","==> epoch 3427: test_loss = 1820.95 \n","==> epoch 3428: train_loss = 1814.96 \n","==> epoch 3428: test_loss = 1821.29 \n","==> epoch 3429: train_loss = 1814.99 \n","==> epoch 3429: test_loss = 1821.26 \n","==> epoch 3430: train_loss = 1815.04 \n","==> epoch 3430: test_loss = 1821.01 \n","==> epoch 3431: train_loss = 1814.95 \n","==> epoch 3431: test_loss = 1821.18 \n","==> epoch 3432: train_loss = 1814.97 \n","==> epoch 3432: test_loss = 1821.07 \n","==> epoch 3433: train_loss = 1815.05 \n","==> epoch 3433: test_loss = 1821.26 \n","==> epoch 3434: train_loss = 1814.92 \n","==> epoch 3434: test_loss = 1821.16 \n","==> epoch 3435: train_loss = 1815.05 \n","==> epoch 3435: test_loss = 1821.36 \n","==> epoch 3436: train_loss = 1815.03 \n","==> epoch 3436: test_loss = 1821.18 \n","==> epoch 3437: train_loss = 1814.99 \n","==> epoch 3437: test_loss = 1821.54 \n","==> epoch 3438: train_loss = 1815.03 \n","==> epoch 3438: test_loss = 1821.06 \n","==> epoch 3439: train_loss = 1814.98 \n","==> epoch 3439: test_loss = 1821.95 \n","==> epoch 3440: train_loss = 1815.03 \n","==> epoch 3440: test_loss = 1820.98 \n","==> epoch 3441: train_loss = 1814.97 \n","==> epoch 3441: test_loss = 1821.13 \n","==> epoch 3442: train_loss = 1814.98 \n","==> epoch 3442: test_loss = 1821.13 \n","==> epoch 3443: train_loss = 1815.00 \n","==> epoch 3443: test_loss = 1821.30 \n","==> epoch 3444: train_loss = 1815.00 \n","==> epoch 3444: test_loss = 1821.09 \n","==> epoch 3445: train_loss = 1814.96 \n","==> epoch 3445: test_loss = 1820.83 \n","==> epoch 3446: train_loss = 1815.01 \n","==> epoch 3446: test_loss = 1821.00 \n","==> epoch 3447: train_loss = 1815.04 \n","==> epoch 3447: test_loss = 1820.87 \n","==> epoch 3448: train_loss = 1815.05 \n","==> epoch 3448: test_loss = 1821.94 \n","==> epoch 3449: train_loss = 1814.97 \n","==> epoch 3449: test_loss = 1821.27 \n","==> epoch 3450: train_loss = 1814.99 \n","==> epoch 3450: test_loss = 1821.13 \n","==> epoch 3451: train_loss = 1815.04 \n","==> epoch 3451: test_loss = 1821.34 \n","==> epoch 3452: train_loss = 1814.96 \n","==> epoch 3452: test_loss = 1821.07 \n","==> epoch 3453: train_loss = 1815.01 \n","==> epoch 3453: test_loss = 1822.88 \n","==> epoch 3454: train_loss = 1815.03 \n","==> epoch 3454: test_loss = 1821.83 \n","==> epoch 3455: train_loss = 1814.97 \n","==> epoch 3455: test_loss = 1821.05 \n","==> epoch 3456: train_loss = 1814.99 \n","==> epoch 3456: test_loss = 1821.30 \n","==> epoch 3457: train_loss = 1815.06 \n","==> epoch 3457: test_loss = 1821.32 \n","==> epoch 3458: train_loss = 1815.03 \n","==> epoch 3458: test_loss = 1821.43 \n","==> epoch 3459: train_loss = 1814.90 \n","==> epoch 3459: test_loss = 1821.33 \n","==> epoch 3460: train_loss = 1814.98 \n","==> epoch 3460: test_loss = 1820.92 \n","==> epoch 3461: train_loss = 1814.99 \n","==> epoch 3461: test_loss = 1821.31 \n","==> epoch 3462: train_loss = 1814.97 \n","==> epoch 3462: test_loss = 1821.14 \n","==> epoch 3463: train_loss = 1814.96 \n","==> epoch 3463: test_loss = 1821.48 \n","==> epoch 3464: train_loss = 1815.05 \n","==> epoch 3464: test_loss = 1820.87 \n","==> epoch 3465: train_loss = 1814.98 \n","==> epoch 3465: test_loss = 1821.05 \n","==> epoch 3466: train_loss = 1815.00 \n","==> epoch 3466: test_loss = 1821.04 \n","==> epoch 3467: train_loss = 1815.01 \n","==> epoch 3467: test_loss = 1821.08 \n","==> epoch 3468: train_loss = 1814.93 \n","==> epoch 3468: test_loss = 1821.18 \n","==> epoch 3469: train_loss = 1814.95 \n","==> epoch 3469: test_loss = 1821.19 \n","==> epoch 3470: train_loss = 1815.06 \n","==> epoch 3470: test_loss = 1821.16 \n","==> epoch 3471: train_loss = 1814.95 \n","==> epoch 3471: test_loss = 1821.37 \n","==> epoch 3472: train_loss = 1815.00 \n","==> epoch 3472: test_loss = 1821.67 \n","==> epoch 3473: train_loss = 1815.04 \n","==> epoch 3473: test_loss = 1821.08 \n","==> epoch 3474: train_loss = 1814.98 \n","==> epoch 3474: test_loss = 1820.82 \n","==> epoch 3475: train_loss = 1814.95 \n","==> epoch 3475: test_loss = 1821.20 \n","==> epoch 3476: train_loss = 1814.99 \n","==> epoch 3476: test_loss = 1821.04 \n","==> epoch 3477: train_loss = 1814.99 \n","==> epoch 3477: test_loss = 1821.05 \n","==> epoch 3478: train_loss = 1815.05 \n","==> epoch 3478: test_loss = 1821.34 \n","==> epoch 3479: train_loss = 1815.01 \n","==> epoch 3479: test_loss = 1821.12 \n","==> epoch 3480: train_loss = 1815.00 \n","==> epoch 3480: test_loss = 1820.95 \n","==> epoch 3481: train_loss = 1814.97 \n","==> epoch 3481: test_loss = 1821.69 \n","==> epoch 3482: train_loss = 1814.91 \n","==> epoch 3482: test_loss = 1821.14 \n","==> epoch 3483: train_loss = 1815.00 \n","==> epoch 3483: test_loss = 1821.25 \n","==> epoch 3484: train_loss = 1815.02 \n","==> epoch 3484: test_loss = 1821.14 \n","==> epoch 3485: train_loss = 1815.03 \n","==> epoch 3485: test_loss = 1822.16 \n","==> epoch 3486: train_loss = 1815.02 \n","==> epoch 3486: test_loss = 1821.23 \n","==> epoch 3487: train_loss = 1814.99 \n","==> epoch 3487: test_loss = 1821.38 \n","==> epoch 3488: train_loss = 1815.08 \n","==> epoch 3488: test_loss = 1821.36 \n","==> epoch 3489: train_loss = 1814.95 \n","==> epoch 3489: test_loss = 1821.00 \n","==> epoch 3490: train_loss = 1815.00 \n","==> epoch 3490: test_loss = 1821.15 \n","==> epoch 3491: train_loss = 1814.99 \n","==> epoch 3491: test_loss = 1821.19 \n","==> epoch 3492: train_loss = 1815.06 \n","==> epoch 3492: test_loss = 1821.61 \n","==> epoch 3493: train_loss = 1815.06 \n","==> epoch 3493: test_loss = 1820.98 \n","==> epoch 3494: train_loss = 1814.95 \n","==> epoch 3494: test_loss = 1821.16 \n","==> epoch 3495: train_loss = 1815.00 \n","==> epoch 3495: test_loss = 1822.01 \n","==> epoch 3496: train_loss = 1814.94 \n","==> epoch 3496: test_loss = 1821.44 \n","==> epoch 3497: train_loss = 1815.00 \n","==> epoch 3497: test_loss = 1821.30 \n","==> epoch 3498: train_loss = 1814.98 \n","==> epoch 3498: test_loss = 1821.50 \n","==> epoch 3499: train_loss = 1815.08 \n","==> epoch 3499: test_loss = 1821.20 \n","==> epoch 3500: train_loss = 1814.96 \n","==> epoch 3500: test_loss = 1822.29 \n","==> epoch 3501: train_loss = 1815.05 \n","==> epoch 3501: test_loss = 1821.05 \n","==> epoch 3502: train_loss = 1815.01 \n","==> epoch 3502: test_loss = 1821.29 \n","==> epoch 3503: train_loss = 1814.96 \n","==> epoch 3503: test_loss = 1822.31 \n","==> epoch 3504: train_loss = 1814.98 \n","==> epoch 3504: test_loss = 1821.22 \n","==> epoch 3505: train_loss = 1814.98 \n","==> epoch 3505: test_loss = 1821.25 \n","==> epoch 3506: train_loss = 1815.00 \n","==> epoch 3506: test_loss = 1821.29 \n","==> epoch 3507: train_loss = 1814.95 \n","==> epoch 3507: test_loss = 1821.11 \n","==> epoch 3508: train_loss = 1815.00 \n","==> epoch 3508: test_loss = 1821.07 \n","==> epoch 3509: train_loss = 1815.04 \n","==> epoch 3509: test_loss = 1821.11 \n","==> epoch 3510: train_loss = 1814.97 \n","==> epoch 3510: test_loss = 1821.19 \n","==> epoch 3511: train_loss = 1815.02 \n","==> epoch 3511: test_loss = 1821.35 \n","==> epoch 3512: train_loss = 1814.98 \n","==> epoch 3512: test_loss = 1821.39 \n","==> epoch 3513: train_loss = 1814.98 \n","==> epoch 3513: test_loss = 1820.96 \n","==> epoch 3514: train_loss = 1815.00 \n","==> epoch 3514: test_loss = 1820.95 \n","==> epoch 3515: train_loss = 1815.00 \n","==> epoch 3515: test_loss = 1821.60 \n","==> epoch 3516: train_loss = 1815.02 \n","==> epoch 3516: test_loss = 1821.39 \n","==> epoch 3517: train_loss = 1814.94 \n","==> epoch 3517: test_loss = 1821.89 \n","==> epoch 3518: train_loss = 1815.02 \n","==> epoch 3518: test_loss = 1821.01 \n","==> epoch 3519: train_loss = 1814.96 \n","==> epoch 3519: test_loss = 1821.45 \n","==> epoch 3520: train_loss = 1815.02 \n","==> epoch 3520: test_loss = 1821.86 \n","==> epoch 3521: train_loss = 1814.99 \n","==> epoch 3521: test_loss = 1821.23 \n","==> epoch 3522: train_loss = 1814.94 \n","==> epoch 3522: test_loss = 1820.95 \n","==> epoch 3523: train_loss = 1814.99 \n","==> epoch 3523: test_loss = 1821.14 \n","==> epoch 3524: train_loss = 1815.03 \n","==> epoch 3524: test_loss = 1821.73 \n","==> epoch 3525: train_loss = 1814.95 \n","==> epoch 3525: test_loss = 1821.12 \n","==> epoch 3526: train_loss = 1814.96 \n","==> epoch 3526: test_loss = 1821.27 \n","==> epoch 3527: train_loss = 1814.94 \n","==> epoch 3527: test_loss = 1821.46 \n","==> epoch 3528: train_loss = 1814.95 \n","==> epoch 3528: test_loss = 1821.33 \n","==> epoch 3529: train_loss = 1814.94 \n","==> epoch 3529: test_loss = 1821.55 \n","==> epoch 3530: train_loss = 1815.06 \n","==> epoch 3530: test_loss = 1821.52 \n","==> epoch 3531: train_loss = 1815.04 \n","==> epoch 3531: test_loss = 1821.25 \n","==> epoch 3532: train_loss = 1814.99 \n","==> epoch 3532: test_loss = 1821.30 \n","==> epoch 3533: train_loss = 1814.97 \n","==> epoch 3533: test_loss = 1821.50 \n","==> epoch 3534: train_loss = 1815.01 \n","==> epoch 3534: test_loss = 1821.52 \n","==> epoch 3535: train_loss = 1814.91 \n","==> epoch 3535: test_loss = 1821.23 \n","==> epoch 3536: train_loss = 1815.06 \n","==> epoch 3536: test_loss = 1821.41 \n","==> epoch 3537: train_loss = 1814.96 \n","==> epoch 3537: test_loss = 1821.70 \n","==> epoch 3538: train_loss = 1815.06 \n","==> epoch 3538: test_loss = 1820.99 \n","==> epoch 3539: train_loss = 1814.95 \n","==> epoch 3539: test_loss = 1821.23 \n","==> epoch 3540: train_loss = 1815.08 \n","==> epoch 3540: test_loss = 1821.13 \n","==> epoch 3541: train_loss = 1814.99 \n","==> epoch 3541: test_loss = 1821.08 \n","==> epoch 3542: train_loss = 1814.98 \n","==> epoch 3542: test_loss = 1821.25 \n","==> epoch 3543: train_loss = 1815.03 \n","==> epoch 3543: test_loss = 1821.19 \n","==> epoch 3544: train_loss = 1814.98 \n","==> epoch 3544: test_loss = 1821.27 \n","==> epoch 3545: train_loss = 1814.91 \n","==> epoch 3545: test_loss = 1821.26 \n","==> epoch 3546: train_loss = 1814.99 \n","==> epoch 3546: test_loss = 1820.90 \n","==> epoch 3547: train_loss = 1815.06 \n","==> epoch 3547: test_loss = 1821.75 \n","==> epoch 3548: train_loss = 1814.96 \n","==> epoch 3548: test_loss = 1821.80 \n","==> epoch 3549: train_loss = 1815.01 \n","==> epoch 3549: test_loss = 1821.44 \n","==> epoch 3550: train_loss = 1814.95 \n","==> epoch 3550: test_loss = 1821.37 \n","==> epoch 3551: train_loss = 1814.96 \n","==> epoch 3551: test_loss = 1821.34 \n","==> epoch 3552: train_loss = 1815.01 \n","==> epoch 3552: test_loss = 1821.76 \n","==> epoch 3553: train_loss = 1814.97 \n","==> epoch 3553: test_loss = 1821.81 \n","==> epoch 3554: train_loss = 1815.04 \n","==> epoch 3554: test_loss = 1821.29 \n","==> epoch 3555: train_loss = 1814.94 \n","==> epoch 3555: test_loss = 1821.33 \n","==> epoch 3556: train_loss = 1814.98 \n","==> epoch 3556: test_loss = 1821.21 \n","==> epoch 3557: train_loss = 1815.00 \n","==> epoch 3557: test_loss = 1822.10 \n","==> epoch 3558: train_loss = 1815.01 \n","==> epoch 3558: test_loss = 1821.88 \n","==> epoch 3559: train_loss = 1814.89 \n","==> epoch 3559: test_loss = 1821.06 \n","==> epoch 3560: train_loss = 1814.98 \n","==> epoch 3560: test_loss = 1820.91 \n","==> epoch 3561: train_loss = 1814.95 \n","==> epoch 3561: test_loss = 1821.66 \n","==> epoch 3562: train_loss = 1814.98 \n","==> epoch 3562: test_loss = 1821.19 \n","==> epoch 3563: train_loss = 1814.98 \n","==> epoch 3563: test_loss = 1821.22 \n","==> epoch 3564: train_loss = 1814.99 \n","==> epoch 3564: test_loss = 1821.15 \n","==> epoch 3565: train_loss = 1814.98 \n","==> epoch 3565: test_loss = 1821.70 \n","==> epoch 3566: train_loss = 1815.06 \n","==> epoch 3566: test_loss = 1820.95 \n","==> epoch 3567: train_loss = 1814.96 \n","==> epoch 3567: test_loss = 1822.04 \n","==> epoch 3568: train_loss = 1814.96 \n","==> epoch 3568: test_loss = 1821.14 \n","==> epoch 3569: train_loss = 1815.02 \n","==> epoch 3569: test_loss = 1821.14 \n","==> epoch 3570: train_loss = 1814.95 \n","==> epoch 3570: test_loss = 1821.18 \n","==> epoch 3571: train_loss = 1815.01 \n","==> epoch 3571: test_loss = 1821.90 \n","==> epoch 3572: train_loss = 1815.00 \n","==> epoch 3572: test_loss = 1821.72 \n","==> epoch 3573: train_loss = 1814.97 \n","==> epoch 3573: test_loss = 1820.96 \n","==> epoch 3574: train_loss = 1814.99 \n","==> epoch 3574: test_loss = 1821.09 \n","==> epoch 3575: train_loss = 1814.95 \n","==> epoch 3575: test_loss = 1821.80 \n","==> epoch 3576: train_loss = 1814.96 \n","==> epoch 3576: test_loss = 1821.44 \n","==> epoch 3577: train_loss = 1815.05 \n","==> epoch 3577: test_loss = 1821.69 \n","==> epoch 3578: train_loss = 1814.97 \n","==> epoch 3578: test_loss = 1821.25 \n","==> epoch 3579: train_loss = 1814.98 \n","==> epoch 3579: test_loss = 1821.71 \n","==> epoch 3580: train_loss = 1814.96 \n","==> epoch 3580: test_loss = 1821.02 \n","==> epoch 3581: train_loss = 1815.02 \n","==> epoch 3581: test_loss = 1820.97 \n","==> epoch 3582: train_loss = 1814.97 \n","==> epoch 3582: test_loss = 1821.48 \n","==> epoch 3583: train_loss = 1814.95 \n","==> epoch 3583: test_loss = 1821.13 \n","==> epoch 3584: train_loss = 1814.92 \n","==> epoch 3584: test_loss = 1821.29 \n","==> epoch 3585: train_loss = 1814.99 \n","==> epoch 3585: test_loss = 1821.20 \n","==> epoch 3586: train_loss = 1815.02 \n","==> epoch 3586: test_loss = 1821.20 \n","==> epoch 3587: train_loss = 1814.99 \n","==> epoch 3587: test_loss = 1821.40 \n","==> epoch 3588: train_loss = 1815.02 \n","==> epoch 3588: test_loss = 1821.72 \n","==> epoch 3589: train_loss = 1815.05 \n","==> epoch 3589: test_loss = 1821.91 \n","==> epoch 3590: train_loss = 1815.03 \n","==> epoch 3590: test_loss = 1821.14 \n","==> epoch 3591: train_loss = 1815.00 \n","==> epoch 3591: test_loss = 1821.10 \n","==> epoch 3592: train_loss = 1814.94 \n","==> epoch 3592: test_loss = 1821.01 \n","==> epoch 3593: train_loss = 1815.03 \n","==> epoch 3593: test_loss = 1821.03 \n","==> epoch 3594: train_loss = 1815.03 \n","==> epoch 3594: test_loss = 1821.79 \n","==> epoch 3595: train_loss = 1815.00 \n","==> epoch 3595: test_loss = 1821.33 \n","==> epoch 3596: train_loss = 1815.01 \n","==> epoch 3596: test_loss = 1821.89 \n","==> epoch 3597: train_loss = 1815.04 \n","==> epoch 3597: test_loss = 1821.55 \n","==> epoch 3598: train_loss = 1814.99 \n","==> epoch 3598: test_loss = 1821.28 \n","==> epoch 3599: train_loss = 1814.97 \n","==> epoch 3599: test_loss = 1821.37 \n","==> epoch 3600: train_loss = 1815.09 \n","==> epoch 3600: test_loss = 1821.12 \n","==> epoch 3601: train_loss = 1815.05 \n","==> epoch 3601: test_loss = 1821.10 \n","==> epoch 3602: train_loss = 1814.96 \n","==> epoch 3602: test_loss = 1821.00 \n","==> epoch 3603: train_loss = 1814.95 \n","==> epoch 3603: test_loss = 1822.62 \n","==> epoch 3604: train_loss = 1814.96 \n","==> epoch 3604: test_loss = 1821.11 \n","==> epoch 3605: train_loss = 1815.00 \n","==> epoch 3605: test_loss = 1821.10 \n","==> epoch 3606: train_loss = 1815.00 \n","==> epoch 3606: test_loss = 1821.58 \n","==> epoch 3607: train_loss = 1814.93 \n","==> epoch 3607: test_loss = 1821.28 \n","==> epoch 3608: train_loss = 1815.03 \n","==> epoch 3608: test_loss = 1821.08 \n","==> epoch 3609: train_loss = 1814.97 \n","==> epoch 3609: test_loss = 1821.07 \n","==> epoch 3610: train_loss = 1815.02 \n","==> epoch 3610: test_loss = 1821.00 \n","==> epoch 3611: train_loss = 1815.05 \n","==> epoch 3611: test_loss = 1821.15 \n","==> epoch 3612: train_loss = 1815.04 \n","==> epoch 3612: test_loss = 1821.42 \n","==> epoch 3613: train_loss = 1814.98 \n","==> epoch 3613: test_loss = 1821.10 \n","==> epoch 3614: train_loss = 1814.97 \n","==> epoch 3614: test_loss = 1821.50 \n","==> epoch 3615: train_loss = 1815.00 \n","==> epoch 3615: test_loss = 1821.05 \n","==> epoch 3616: train_loss = 1815.00 \n","==> epoch 3616: test_loss = 1821.28 \n","==> epoch 3617: train_loss = 1814.95 \n","==> epoch 3617: test_loss = 1821.09 \n","==> epoch 3618: train_loss = 1814.96 \n","==> epoch 3618: test_loss = 1821.43 \n","==> epoch 3619: train_loss = 1815.05 \n","==> epoch 3619: test_loss = 1821.06 \n","==> epoch 3620: train_loss = 1815.10 \n","==> epoch 3620: test_loss = 1821.16 \n","==> epoch 3621: train_loss = 1814.96 \n","==> epoch 3621: test_loss = 1821.29 \n","==> epoch 3622: train_loss = 1815.00 \n","==> epoch 3622: test_loss = 1821.14 \n","==> epoch 3623: train_loss = 1814.98 \n","==> epoch 3623: test_loss = 1821.48 \n","==> epoch 3624: train_loss = 1815.00 \n","==> epoch 3624: test_loss = 1821.24 \n","==> epoch 3625: train_loss = 1814.99 \n","==> epoch 3625: test_loss = 1821.43 \n","==> epoch 3626: train_loss = 1815.02 \n","==> epoch 3626: test_loss = 1821.24 \n","==> epoch 3627: train_loss = 1814.94 \n","==> epoch 3627: test_loss = 1821.13 \n","==> epoch 3628: train_loss = 1814.93 \n","==> epoch 3628: test_loss = 1821.03 \n","==> epoch 3629: train_loss = 1815.01 \n","==> epoch 3629: test_loss = 1821.35 \n","==> epoch 3630: train_loss = 1815.01 \n","==> epoch 3630: test_loss = 1821.02 \n","==> epoch 3631: train_loss = 1815.06 \n","==> epoch 3631: test_loss = 1821.43 \n","==> epoch 3632: train_loss = 1814.92 \n","==> epoch 3632: test_loss = 1821.10 \n","==> epoch 3633: train_loss = 1814.98 \n","==> epoch 3633: test_loss = 1821.20 \n","==> epoch 3634: train_loss = 1814.95 \n","==> epoch 3634: test_loss = 1821.71 \n","==> epoch 3635: train_loss = 1815.02 \n","==> epoch 3635: test_loss = 1821.11 \n","==> epoch 3636: train_loss = 1815.03 \n","==> epoch 3636: test_loss = 1821.69 \n","==> epoch 3637: train_loss = 1814.94 \n","==> epoch 3637: test_loss = 1822.65 \n","==> epoch 3638: train_loss = 1815.05 \n","==> epoch 3638: test_loss = 1821.19 \n","==> epoch 3639: train_loss = 1814.96 \n","==> epoch 3639: test_loss = 1821.42 \n","==> epoch 3640: train_loss = 1814.98 \n","==> epoch 3640: test_loss = 1821.33 \n","==> epoch 3641: train_loss = 1814.96 \n","==> epoch 3641: test_loss = 1821.04 \n","==> epoch 3642: train_loss = 1815.00 \n","==> epoch 3642: test_loss = 1821.24 \n","==> epoch 3643: train_loss = 1815.08 \n","==> epoch 3643: test_loss = 1821.14 \n","==> epoch 3644: train_loss = 1815.03 \n","==> epoch 3644: test_loss = 1821.18 \n","==> epoch 3645: train_loss = 1814.91 \n","==> epoch 3645: test_loss = 1821.85 \n","==> epoch 3646: train_loss = 1815.02 \n","==> epoch 3646: test_loss = 1820.86 \n","==> epoch 3647: train_loss = 1815.08 \n","==> epoch 3647: test_loss = 1821.30 \n","==> epoch 3648: train_loss = 1814.91 \n","==> epoch 3648: test_loss = 1821.29 \n","==> epoch 3649: train_loss = 1814.98 \n","==> epoch 3649: test_loss = 1821.25 \n","==> epoch 3650: train_loss = 1815.05 \n","==> epoch 3650: test_loss = 1821.22 \n","==> epoch 3651: train_loss = 1814.93 \n","==> epoch 3651: test_loss = 1821.58 \n","==> epoch 3652: train_loss = 1815.01 \n","==> epoch 3652: test_loss = 1821.15 \n","==> epoch 3653: train_loss = 1815.05 \n","==> epoch 3653: test_loss = 1821.02 \n","==> epoch 3654: train_loss = 1814.98 \n","==> epoch 3654: test_loss = 1821.59 \n","==> epoch 3655: train_loss = 1814.96 \n","==> epoch 3655: test_loss = 1821.14 \n","==> epoch 3656: train_loss = 1815.03 \n","==> epoch 3656: test_loss = 1821.50 \n","==> epoch 3657: train_loss = 1814.92 \n","==> epoch 3657: test_loss = 1821.14 \n","==> epoch 3658: train_loss = 1814.98 \n","==> epoch 3658: test_loss = 1821.34 \n","==> epoch 3659: train_loss = 1815.03 \n","==> epoch 3659: test_loss = 1821.39 \n","==> epoch 3660: train_loss = 1814.99 \n","==> epoch 3660: test_loss = 1822.05 \n","==> epoch 3661: train_loss = 1814.97 \n","==> epoch 3661: test_loss = 1821.45 \n","==> epoch 3662: train_loss = 1814.98 \n","==> epoch 3662: test_loss = 1822.94 \n","==> epoch 3663: train_loss = 1815.05 \n","==> epoch 3663: test_loss = 1821.13 \n","==> epoch 3664: train_loss = 1814.99 \n","==> epoch 3664: test_loss = 1821.39 \n","==> epoch 3665: train_loss = 1815.01 \n","==> epoch 3665: test_loss = 1821.72 \n","==> epoch 3666: train_loss = 1814.96 \n","==> epoch 3666: test_loss = 1821.59 \n","==> epoch 3667: train_loss = 1814.97 \n","==> epoch 3667: test_loss = 1821.07 \n","==> epoch 3668: train_loss = 1815.00 \n","==> epoch 3668: test_loss = 1821.34 \n","==> epoch 3669: train_loss = 1815.01 \n","==> epoch 3669: test_loss = 1821.28 \n","==> epoch 3670: train_loss = 1815.04 \n","==> epoch 3670: test_loss = 1821.09 \n","==> epoch 3671: train_loss = 1814.96 \n","==> epoch 3671: test_loss = 1821.41 \n","==> epoch 3672: train_loss = 1814.97 \n","==> epoch 3672: test_loss = 1821.19 \n","==> epoch 3673: train_loss = 1815.00 \n","==> epoch 3673: test_loss = 1821.09 \n","==> epoch 3674: train_loss = 1814.94 \n","==> epoch 3674: test_loss = 1822.56 \n","==> epoch 3675: train_loss = 1815.10 \n","==> epoch 3675: test_loss = 1822.38 \n","==> epoch 3676: train_loss = 1815.05 \n","==> epoch 3676: test_loss = 1821.37 \n","==> epoch 3677: train_loss = 1814.99 \n","==> epoch 3677: test_loss = 1821.17 \n","==> epoch 3678: train_loss = 1814.91 \n","==> epoch 3678: test_loss = 1821.32 \n","==> epoch 3679: train_loss = 1815.06 \n","==> epoch 3679: test_loss = 1821.01 \n","==> epoch 3680: train_loss = 1814.97 \n","==> epoch 3680: test_loss = 1821.20 \n","==> epoch 3681: train_loss = 1814.93 \n","==> epoch 3681: test_loss = 1821.28 \n","==> epoch 3682: train_loss = 1814.97 \n","==> epoch 3682: test_loss = 1821.23 \n","==> epoch 3683: train_loss = 1814.99 \n","==> epoch 3683: test_loss = 1821.08 \n","==> epoch 3684: train_loss = 1815.02 \n","==> epoch 3684: test_loss = 1820.93 \n","==> epoch 3685: train_loss = 1814.98 \n","==> epoch 3685: test_loss = 1820.74 \n","==> epoch 3686: train_loss = 1814.98 \n","==> epoch 3686: test_loss = 1822.92 \n","==> epoch 3687: train_loss = 1815.01 \n","==> epoch 3687: test_loss = 1821.34 \n"]}],"source":["main(dataset = \"cifar10\", batch_size = 128, epochs = 4000, sample_size = 64, latent_dim = 100, lr = 1e-3, continue_from = 3250)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"VAE_Project.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fa761cc6025c40e0943c6d0be1729a38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ec3e5c3fc94d4afa8bdefd39776896ff","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6103acd3a3524c52a4a5ae79b953d228","IPY_MODEL_e46b2168742f40c0b6482461f1c6ec87","IPY_MODEL_7ef82d752ce14379b28f923656bea697"]}},"ec3e5c3fc94d4afa8bdefd39776896ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6103acd3a3524c52a4a5ae79b953d228":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_22226cc7c36e40ceaaab9bb1a4b1dcb6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b2618cb15bd24dbe8b1ed7521cbe7ddf"}},"e46b2168742f40c0b6482461f1c6ec87":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b8f8befc40334b93a3392e936b549252","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e3bdcb6a2324f9aa9797f11205538fe"}},"7ef82d752ce14379b28f923656bea697":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_58f959661f5141dc97a6a4ec10f87684","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:05&lt;00:00, 32779221.82it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9a76cf42703547dbac9cda50d36f5726"}},"22226cc7c36e40ceaaab9bb1a4b1dcb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b2618cb15bd24dbe8b1ed7521cbe7ddf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8f8befc40334b93a3392e936b549252":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6e3bdcb6a2324f9aa9797f11205538fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"58f959661f5141dc97a6a4ec10f87684":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9a76cf42703547dbac9cda50d36f5726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bd1f8a975e6c44ca97c0f90e0c20c8d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9d774cddd7d042f889f79d490f26eee5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_42984c36f0eb45f49750e5052ea9ff4e","IPY_MODEL_fe3dae1766694231a3f9bbab713f648c","IPY_MODEL_6c93e7bfa72248a6a3270787a4fcfd96"]}},"9d774cddd7d042f889f79d490f26eee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42984c36f0eb45f49750e5052ea9ff4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2f9c753e6c1643ebacbe35fb54268ada","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5f0126280124811b09ea7f25727a28c"}},"fe3dae1766694231a3f9bbab713f648c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_aea01ef94973470380a20a44d9abc585","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df08e359128c404d9d6df0d7ec86d1a3"}},"6c93e7bfa72248a6a3270787a4fcfd96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f9cc41387f0a4622aabc55bb84729827","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:06&lt;00:00, 32870946.94it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_609a16fc5c724553a8b5d47f5ee32aa8"}},"2f9c753e6c1643ebacbe35fb54268ada":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a5f0126280124811b09ea7f25727a28c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aea01ef94973470380a20a44d9abc585":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"df08e359128c404d9d6df0d7ec86d1a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f9cc41387f0a4622aabc55bb84729827":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"609a16fc5c724553a8b5d47f5ee32aa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}